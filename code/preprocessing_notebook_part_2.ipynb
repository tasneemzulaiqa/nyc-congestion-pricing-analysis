{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0adac3",
   "metadata": {},
   "source": [
    "# Data Cleaning and Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f3d93",
   "metadata": {},
   "source": [
    "This notebook performs ETL (Extract, Transform, Load) on NYC taxi and FHV datasets:  \n",
    "- Cleans and standardizes raw data  \n",
    "- Aligns column names and data types  \n",
    "- Handles missing or invalid values  \n",
    "- Aggregates metrics like fare per mile, fare per minute, average speed  \n",
    "- Flags CBD trips, peak hours, and weekends  \n",
    "- Saves cleaned and curated Parquet files for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for downloading data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, unix_timestamp, round, abs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import builtins\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f393e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_shape(service_type, year, months, base_dir=\"../data/tlc_data/raw/cleaned\"):\n",
    "    total_rows = 0\n",
    "    num_features = None\n",
    "    \n",
    "    for month in months:\n",
    "        folder_path = f\"{base_dir}/{service_type}/{year}-{str(month).zfill(2)}\"\n",
    "        df = spark.read.parquet(folder_path)\n",
    "        total_rows += df.count()\n",
    "        \n",
    "        if num_features is None:\n",
    "            num_features = len(df.columns)\n",
    "    \n",
    "    return {\"service\": service_type, \"year\": year, \"rows\": total_rows, \"features\": num_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cab65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/02 16:06:52 WARN Utils: Your hostname, Tasneems-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.20.7 instead (on interface en0)\n",
      "25/09/02 16:06:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/02 16:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/02 16:06:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51273)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/tasneemzulaiqa/Documents/GitHub/project-1-individual-tasneemzulaiqa/proj1_env/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/tasneemzulaiqa/Documents/GitHub/project-1-individual-tasneemzulaiqa/proj1_env/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/tasneemzulaiqa/Documents/GitHub/project-1-individual-tasneemzulaiqa/proj1_env/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tasneemzulaiqa/Documents/GitHub/project-1-individual-tasneemzulaiqa/proj1_env/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"TLC eda\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e42eb5",
   "metadata": {},
   "source": [
    "## 1. Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64902975",
   "metadata": {},
   "source": [
    "### a. Yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_fare(trip_distance, trip_duration_min, ratecode_id, avg_speed):\n",
    "    \"\"\"\n",
    "    Estimate taxi fare based on trip distance, duration, rate code, and average speed.\n",
    "\n",
    "    - Base fare: $3.00\n",
    "    - Distance-based if avg_speed >= 12 mph; else waiting-time based\n",
    "    - Adjust fare by ratecode:\n",
    "        1 = standard, 2 = JFK flat $70, 3 = +$20, 4 = out-of-city x1.5\n",
    "    - Returns fare rounded to 2 decimals\n",
    "    \"\"\"\n",
    "    base_fare = 3.00\n",
    "    jfk_manhattan = 70.0  \n",
    "\n",
    "    # Calculate standard fare based on distance and speed\n",
    "    if avg_speed >= 12:\n",
    "        distance_fare = 0.70 * (trip_distance / 0.2)\n",
    "        waiting_fare = 0.0\n",
    "    else:\n",
    "        distance_fare = 0.0\n",
    "        waiting_fare = 0.70 * trip_duration_min\n",
    "\n",
    "    standard_fare = base_fare + distance_fare + waiting_fare\n",
    "\n",
    "    ## Out-of-city (Nassau/Westerchester)\n",
    "    if ratecode_id == 4:  #\n",
    "        estimated_fare = standard_fare * 1.5\n",
    "        # Standard city rate\n",
    "    elif ratecode_id == 1: \n",
    "        estimated_fare = standard_fare\n",
    "        # JFK Manhattan\n",
    "    elif ratecode_id == 2:  \n",
    "        estimated_fare = jfk_manhattan\n",
    "        # EWR\n",
    "    elif ratecode_id == 3:  \n",
    "        estimated_fare = standard_fare + 20\n",
    "    else:\n",
    "        estimated_fare = None\n",
    "\n",
    "    if estimated_fare is not None:\n",
    "        return builtins.round(estimated_fare, 2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "estimate_fare_udf = udf(estimate_fare, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change year to preprocess each year\n",
    "YEAR = \"2025\"  \n",
    "MONTHS = range(1, 7)  \n",
    "TLC_OUTPUT_DIR = '../data/tlc_data/'\n",
    "\n",
    "def clean_yellow(input_path, zones_df, year, month):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean and preprocess NYC yellow taxi data for a given year and month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # Calculate trip time in minutes, rounded to 2 decimals\n",
    "    df = df.withColumn(\"trip_time\", round((unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60, 2))\n",
    "    \n",
    "    # Map pickup zones\n",
    "    pu_zones = zones_df.select(\n",
    "        F.col(\"LocationID\").alias(\"pu_location_id\"),\n",
    "        F.col(\"Zone\").alias(\"pu_zone\"),\n",
    "        F.col(\"Borough\").alias(\"pu_borough\"),\n",
    "        F.col(\"service_zone\").alias(\"pu_service_zone\")\n",
    "    )\n",
    "    \n",
    "    df = df.join(pu_zones, on=\"pu_location_id\", how=\"left\")\n",
    "    \n",
    "    # Map drop-off zones (rename columns for joining)\n",
    "    do_zones = zones_df.select(\n",
    "        F.col(\"LocationID\").alias(\"do_location_id\"),\n",
    "        F.col(\"Zone\").alias(\"do_zone\"),\n",
    "        F.col(\"Borough\").alias(\"do_borough\"),\n",
    "        F.col(\"service_zone\").alias(\"do_service_zone\")\n",
    "    )\n",
    "    df = df.join(do_zones, on=\"do_location_id\", how=\"left\")\n",
    "    \n",
    "    # Drop service zones columns \n",
    "    df = df.drop(\"pu_service_zone\", \"do_service_zone\")\n",
    "    \n",
    "    # Filter trip distance outliers\n",
    "    df = df.filter((F.col(\"trip_miles\") > 0.0) & (F.col(\"trip_miles\") <= 200.0))\n",
    "    \n",
    "    # Filter trip time outliers\n",
    "    df = df.filter((F.col(\"trip_time\") > 0.0) & (F.col(\"trip_time\") < 300.0))\n",
    "    \n",
    "    # Calculate average speed mph\n",
    "    df = df.withColumn(\"avg_speed_mph\", round(F.col(\"trip_miles\") / (F.col(\"trip_time\") / 60), 2))\n",
    "    \n",
    "    # Filter unreasonable speeds\n",
    "    df = df.filter((F.col(\"avg_speed_mph\") >= 3) & (F.col(\"avg_speed_mph\") < 80))\n",
    "    \n",
    "    # Filter voided trips\n",
    "    df = df.filter(F.col(\"payment_type\") != 6)\n",
    "    \n",
    "    \n",
    "    # Filter low fare amount\n",
    "    df = df.filter(F.col(\"fare_amount\") > 3)\n",
    "    \n",
    "    # Calculate estimated fare (assuming you have defined estimate_fare_udf)\n",
    "    df = df.withColumn(\"estimated_fare\", estimate_fare_udf(F.col(\"trip_miles\"), F.col(\"trip_time\"), F.col(\"ratecode_id\"), F.col(\"avg_speed_mph\")))\n",
    "    \n",
    "    # Calculate absolute fare difference\n",
    "    df = df.withColumn(\"fare_diff\", F.abs(F.col(\"fare_amount\") - F.col(\"estimated_fare\")))\n",
    "    \n",
    "    # Filter where estimated fare exists and difference <= 100\n",
    "    df = df.filter((F.col(\"estimated_fare\").isNotNull()) & (F.col(\"fare_diff\") <= 100.0))\n",
    "    \n",
    "    # Filter passenger count\n",
    "    df = df.filter((F.col(\"passenger_count\") >= 1) & (F.col(\"passenger_count\") < 6))\n",
    "    \n",
    "    # extra for 2025 data / comment the code for 2024 where \n",
    "    \n",
    "    df = df.filter(F.col(\"cbd_congestion_fee\") >= 0.0)\n",
    "    \n",
    "    # Filter valid pickup and dropoff zones\n",
    "    df = df.filter(\n",
    "        (F.col(\"pu_zone\").isNotNull()) &\n",
    "        (~F.col(\"pu_zone\").isin(\"NV\", \"NA\")) &\n",
    "        (F.col(\"do_zone\").isNotNull()) &\n",
    "        (~F.col(\"do_zone\").isin(\"NV\", \"NA\"))\n",
    "    )\n",
    "    output_path = f'{TLC_OUTPUT_DIR}raw/cleaned/yellow/{year}-{str(month).zfill(2)}'\n",
    "    df.coalesce(1).write.mode('overwrite').parquet(output_path)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898ba51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/02 16:07:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zones_df = spark.read.csv(\"../data/taxi_zones/taxi+_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "for month in MONTHS:\n",
    "    input_path = f'{TLC_OUTPUT_DIR}raw/yellow/{YEAR}-{str(month).zfill(2)}'\n",
    "    clean_yellow(input_path, zones_df, YEAR, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ea55d",
   "metadata": {},
   "source": [
    "### b. HVFHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58211ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change year to preprocess each year\n",
    "YEAR = \"2025\"  \n",
    "MONTHS = range(1, 7)  \n",
    "TLC_OUTPUT_DIR = '../data/tlc_data/'\n",
    "\n",
    "def clean_hvfhv(input_path, zones_df, year, month):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean and preprocess NYC hvfhv data for a given year and month\n",
    "    \"\"\"\n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # Map pickup zones\n",
    "    pu_zones = zones_df.select(\n",
    "        F.col(\"LocationID\").alias(\"pu_location_id\"),\n",
    "        F.col(\"Zone\").alias(\"pu_zone\"),\n",
    "        F.col(\"Borough\").alias(\"pu_borough\"),\n",
    "        F.col(\"service_zone\").alias(\"pu_service_zone\")\n",
    "    )\n",
    "    df = df.join(pu_zones, on=\"pu_location_id\", how=\"left\")\n",
    "    \n",
    "    # Map drop-off zones\n",
    "    do_zones = zones_df.select(\n",
    "        F.col(\"LocationID\").alias(\"do_location_id\"),\n",
    "        F.col(\"Zone\").alias(\"do_zone\"),\n",
    "        F.col(\"Borough\").alias(\"do_borough\"),\n",
    "        F.col(\"service_zone\").alias(\"do_service_zone\")\n",
    "    )\n",
    "    df = df.join(do_zones, on=\"do_location_id\", how=\"left\")\n",
    "    \n",
    "    # Drop service zone columns\n",
    "    df = df.drop(\"pu_service_zone\", \"do_service_zone\")\n",
    "    \n",
    "    # Convert trip_time from seconds to minutes\n",
    "    df = df.withColumn(\"trip_time\", F.round(F.col(\"trip_time\") / 60, 2))\n",
    "    \n",
    "    # Remove nulls / NaNs in numeric columns\n",
    "    numeric_cols = [\"trip_miles\", \"trip_time\", \"fare_amount\", \"driver_pay\",\n",
    "                    \"tolls\", \"tips\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "    for col in numeric_cols:\n",
    "        df = df.filter((F.col(col).isNotNull()) & (~F.isnan(F.col(col))))\n",
    "    \n",
    "    # Remove nulls in datetime columns\n",
    "    datetime_cols = ['request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']\n",
    "    for col in datetime_cols:\n",
    "        df = df.filter(F.col(col).isNotNull())\n",
    "    \n",
    "    # Remove nulls in string/categorical columns\n",
    "    string_cols = [\"pu_zone\", \"do_zone\", \"pu_borough\", \"do_borough\",\n",
    "                   'originating_base_num', 'dispatching_base_num', 'hvfhs_license_num']\n",
    "    for col in string_cols:\n",
    "        df = df.filter(F.col(col).isNotNull())\n",
    "    \n",
    "    # Filter trip distance outliers\n",
    "    df = df.filter((F.col(\"trip_miles\") > 0.0) & (F.col(\"trip_miles\") <= 200.0))\n",
    "    \n",
    "    # Filter trip time outliers\n",
    "    df = df.filter((F.col(\"trip_time\") > 0.0) & (F.col(\"trip_time\") < 300.0))\n",
    "    \n",
    "    # Calculate average speed (mph)\n",
    "    df = df.withColumn(\"avg_speed_mph\", F.round(F.col(\"trip_miles\") / (F.col(\"trip_time\") / 60), 2))\n",
    "    \n",
    "    # Filter unreasonable speeds\n",
    "    df = df.filter((F.col(\"avg_speed_mph\") >= 3) & (F.col(\"avg_speed_mph\") < 80))\n",
    "    \n",
    "    # Filter low fares and driver pay\n",
    "    df = df.filter((F.col(\"fare_amount\") > 3.0) & (F.col(\"driver_pay\") > 0.0))\n",
    "    \n",
    "    df = df.filter(F.col(\"cbd_congestion_fee\") >= 0.0)\n",
    "    \n",
    "    # Filter invalid pickup and dropoff zones\n",
    "    df = df.filter(\n",
    "        (~F.col(\"pu_zone\").isin(\"NV\", \"NA\")) &\n",
    "        (~F.col(\"do_zone\").isin(\"NV\", \"NA\"))\n",
    "    )\n",
    "\n",
    "    # Save cleaned data\n",
    "    output_path = f'{TLC_OUTPUT_DIR}raw/cleaned/hvfhv/{year}-{str(month).zfill(2)}'\n",
    "    df.coalesce(1).write.mode('overwrite').parquet(output_path)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c75f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for month in MONTHS:\n",
    "    input_path = f'{TLC_OUTPUT_DIR}raw/hvfhv/{YEAR}-{str(month).zfill(2)}'\n",
    "    clean_hvfhv(input_path, zones_df, YEAR, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124770d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YELLOW 2024\n",
      "  Total rows: 16,868,542\n",
      "  Features: 27\n",
      "----------------------------------------\n",
      "YELLOW 2025\n",
      "  Total rows: 16,918,186\n",
      "  Features: 28\n",
      "----------------------------------------\n",
      "HVFHV 2024\n",
      "  Total rows: 86,058,824\n",
      "  Features: 29\n",
      "----------------------------------------\n",
      "HVFHV 2025\n",
      "  Total rows: 83,670,510\n",
      "  Features: 30\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "hvfhv_2024 = get_shape(\"hvfhv\", 2024, range(1, 7))\n",
    "hvfhv_2025 = get_shape(\"hvfhv\", 2025, range(1, 7))\n",
    "yellow_2024 = get_shape(\"yellow\", 2024, range(1, 7))\n",
    "yellow_2025 = get_shape(\"yellow\", 2025, range(1, 7))\n",
    "\n",
    "for s in [yellow_2024, yellow_2025, hvfhv_2024, hvfhv_2025]:\n",
    "    print(f\"{s['service'].upper()} {s['year']}\")\n",
    "    print(f\"  Total rows: {s['rows']:,}\")\n",
    "    print(f\"  Features: {s['features']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0fb7f",
   "metadata": {},
   "source": [
    "## 2. Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd_zones = {50, 48, 163, 230, 161, 162, 229, 233, 170, 164, 100, 186, 68, 246,\n",
    "            90, 234, 107, 137, 224, 158, 249, 114, 113, 79, 4, 125, 211, 144,\n",
    "            148, 232, 45, 231, 13, 209, 87, 88, 12, 261}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e4bbc",
   "metadata": {},
   "source": [
    "### a. HVFHV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2024\"\n",
    "MONTHS = range(1, 7)\n",
    "TLC_INPUT_DIR = '../data/tlc_data/raw/cleaned/'\n",
    "TLC_OUTPUT_DIR = '../data/tlc_data/raw/cleaned/curated/'\n",
    "\n",
    "def aggregate_hvfhv(input_path, year, month):\n",
    "    \n",
    "    \"\"\"\n",
    "    Aggregates NYC hvfhv data for a given year and month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # Calculate total_amount\n",
    "    df = df.withColumn(\n",
    "        \"total_amount\",\n",
    "        F.round(\n",
    "            (F.col(\"fare_amount\") + F.col(\"tolls\") + F.col(\"bcf\") + F.col(\"sales_tax\") +\n",
    "             F.col(\"congestion_surcharge\") + F.col(\"airport_fee\") + F.col(\"tips\") + F.col(\"driver_pay\")), 2)\n",
    "    )\n",
    "    \n",
    "    # Calculate total per mile \n",
    "    df = df.withColumn(\n",
    "        \"fare_per_mile\",\n",
    "        F.when(F.col(\"trip_miles\") > 0, F.round(F.col(\"fare_amount\") / F.col(\"trip_miles\"), 2)).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Calculate total per minute\n",
    "    df = df.withColumn(\n",
    "        \"fare_per_min\",\n",
    "        F.when(F.col(\"trip_time\") > 0, F.round(F.col(\"fare_amount\") / F.col(\"trip_time\"), 2)).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # add cbd flag\n",
    "    if (year == \"2024\"):   \n",
    "        df = df.withColumn(\"is_cbd\",F.when((F.col(\"pu_location_id\").isin(cbd_zones)) | (F.col(\"do_location_id\").isin(cbd_zones)), 1).otherwise(0))\n",
    "    else:   \n",
    "        df = df.withColumn(\"is_cbd\", F.when(F.col(\"cbd_congestion_fee\") > 0, 1).otherwise(0))\n",
    "    \n",
    "    \n",
    "    # Extract day_of_week, is_weekend\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(\"pickup_datetime\"))\n",
    "    df = df.withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1,7]),1).otherwise(0))\n",
    "    \n",
    "    df = df.withColumn(\"is_peak\",F.when((F.hour(\"pickup_datetime\").between(6, 10)) | (F.hour(\"pickup_datetime\").between(16, 20)), 1).otherwise(0))\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(f'{TLC_OUTPUT_DIR}hvfhv/{year}-{str(month).zfill(2)}')\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f82fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for month in MONTHS:\n",
    "    input_path = f'{TLC_INPUT_DIR}hvfhv/{YEAR}-{str(month).zfill(2)}'\n",
    "    aggregate_hvfhv(input_path, YEAR, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0513a",
   "metadata": {},
   "source": [
    "### b. Yellow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2025\"\n",
    "MONTHS = range(1, 7)\n",
    "TLC_INPUT_DIR = '../data/tlc_data/raw/cleaned/'\n",
    "TLC_OUTPUT_DIR = '../data/tlc_data/raw/cleaned/curated/'\n",
    "\n",
    "def aggregate_yellow(input_path, year, month):\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    Aggregates NYC Yellow taxi data for a given year and month\n",
    "    \"\"\"\n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(input_path)\n",
    "    \n",
    "    df = df.drop(\"fare_diff\", \"estimated_fare\")\n",
    "    \n",
    "    # Calculate fare per mile \n",
    "    df = df.withColumn(\n",
    "        \"fare_per_mile\",\n",
    "        F.when(F.col(\"trip_miles\") > 0, F.round(F.col(\"fare_amount\") / F.col(\"trip_miles\"), 2)).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Calculate fare per minute\n",
    "    df = df.withColumn(\n",
    "        \"fare_per_min\",\n",
    "        F.when(F.col(\"trip_time\") > 0, F.round(F.col(\"fare_amount\") / F.col(\"trip_time\"), 2)).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # add cbd flag\n",
    "    if (year == \"2024\"):   \n",
    "        df = df.withColumn(\"is_cbd\",F.when((F.col(\"pu_location_id\").isin(cbd_zones)) | (F.col(\"do_location_id\").isin(cbd_zones)), 1).otherwise(0))\n",
    "    else:   \n",
    "        df = df.withColumn(\"is_cbd\", F.when(F.col(\"cbd_congestion_fee\") > 0, 1).otherwise(0))\n",
    "    \n",
    "    \n",
    "    # Extract day_of_week, is_weekend\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(\"pickup_datetime\"))\n",
    "    \n",
    "    df = df.withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1,7]),1).otherwise(0))\n",
    "    \n",
    "    df = df.withColumn(\"is_peak\",F.when((F.hour(\"pickup_datetime\").between(6, 10)) | (F.hour(\"pickup_datetime\").between(16, 20)), 1).otherwise(0))\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(f'{TLC_OUTPUT_DIR}yellow/{year}-{str(month).zfill(2)}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6a3d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for month in MONTHS:\n",
    "    input_path = f'{TLC_INPUT_DIR}yellow/{YEAR}-{str(month).zfill(2)}'\n",
    "    aggregate_yellow(input_path, YEAR, month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
